{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> https://blog.csdn.net/qq_25737169/article/details/78847691"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 为什么要使用梯度更新规则"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设下图为损失函数的数据空间，我们最优的权值就是为了寻找下图中的最小值点，对于数学寻找最小值问题，采用梯度下降的方法刚刚好。\n",
    "\n",
    "![损失函数数据空间](photo/losspic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度消失，梯度爆炸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 从深层网络角度\n",
    "\n",
    "![深层网络](photo/梯度.png)\n",
    "\n",
    "简单来说，就是根据链式求导法则求导时，对激活函数进行求导，如果此部分大于1，那么随着层数的增多，最终求出的梯度更新将以指数形式增加，即**梯度爆炸**；当梯度以指数形式衰减，即**梯度消失**\n",
    "\n",
    "![隐层权重更新](photo/hiddenlayer.png)\n",
    "\n",
    "从深层网络角度来讲，**不同的层学习的速度差异很大，表现为网络中靠近输出的层学习的情况很好，靠近输入的层学习的很慢，**有时甚至训练了很久，前几层的权值和刚开始随机初始化的值差不多。因此，梯度消失、爆炸，其根本原因在于反向传播训练法则，属于先天不足\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "2. 激活函数角度\n",
    "\n",
    "如果使用**sigmod**函数作为激活函数，则梯度不可能超过0.25。所以经过链式求导之后，很容易发生梯度消失。\n",
    "\n",
    "![sigmod](photo/sigmod.jpeg)\n",
    "![sigmodDerivative](photo/derivative.png)\n",
    "\n",
    "**tanh**作为激活函数的话：比sigmod好一点，但是倒数仍然小于1。\n",
    "\n",
    "![tanh](photo/tanh.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 解决方案"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **预训练加微调** ：此方法来自Hinton在2006年发表的一篇论文，Hinton为了解决梯度的问题，提出采取无监督逐层训练方法，其基本思想是每次训练一层隐节点，训练时将上一层隐节点的输出作为输入，而本层隐节点的输出作为下一层隐节点的输入，此过程就是逐层“预训练”（pre-training）；在预训练完成后，再对整个网络进行“微调”（fine-tunning）。**此思想相当于是先寻找局部最优，然后整合起来寻找全局最优，**此方法有一定的好处，但是目前应用的不是很多了。\n",
    "\n",
    "<br>\n",
    "\n",
    "> 梯度剪切：设置一个梯度剪切阀值，更新梯度的时候，如果梯度超过这个阀值，将其强制限制在这个范围之内，防止梯度爆炸。\n",
    "\n",
    "\n",
    "> **正则**：\n",
    "![正则](photo/正则.png)\n",
    "\n",
    "**relu激活函数**\n",
    "rule函数激活函数x>0情况下始终为1，所以不存在梯度爆炸消失的问题，每层的网络都可以得到相同的更新速度。\n",
    "![relu](photo/relu.png)\n",
    "\n",
    "由于负数部分恒为0，会导致一些神经元无法激活，(可通过设置小学习率部分解决)\n",
    "输出不是以0为中心的。\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**leakrelu**\n",
    "![leakrelu](photo/leakrelu.png)\n",
    "\n",
    "**elu**\n",
    "![elu](photo/elu.png)\n",
    "\n",
    "\n",
    "还有两种解决方案： batchnorm and 残差结构\n",
    "http://blog.csdn.net/qq_25737169/article/details/79048516 batch normalization \n",
    "\n",
    "Deep Residual Learning for Image Recognition 残差结构相关论文\n",
    "https://zhuanlan.zhihu.com/p/31852747 简单介绍残差如何解决梯度的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 激活函数以零为中心的影响\n",
    "\n",
    "![zero-centered](photo/zero-centered.png)\n",
    "\n",
    "模型走绿色箭头收敛最快，但是当Sigmod函数作为激活函数的神经网络，由于输入值的符号为正，所以模型参数可能走类似红色的箭头，收敛速度大大减小。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
