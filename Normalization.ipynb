{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性变化的性质\n",
    "\n",
    "> 线性变化不改变原始数据的数值排序"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![归一化标准化](photo/1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_x:  [[-1.1124854 ]\n",
      " [-0.93448773]\n",
      " [ 1.82447605]\n",
      " [ 0.66749124]\n",
      " [-0.31149591]\n",
      " [-0.13349825]]\n",
      "norm_x: [[0.        ]\n",
      " [0.06060606]\n",
      " [1.        ]\n",
      " [0.60606061]\n",
      " [0.27272727]\n",
      " [0.33333333]]\n",
      "原始顺序：  [1. 2. 6. 5. 3. 4.]\n",
      "标准化顺序：  [1. 2. 6. 5. 3. 4.]\n",
      "归一化顺序:  [1. 2. 6. 5. 3. 4.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "x=[[1],[3],[34],[21],[10],[12]]\n",
    "std_x=preprocessing.StandardScaler().fit_transform(x)\n",
    "norm_x=preprocessing.MinMaxScaler().fit_transform(x)\n",
    "print(\"std_x: \",std_x)\n",
    "print(\"norm_x: {0}\".format(norm_x))\n",
    "print('原始顺序： ',rankdata(x))\n",
    "print('标准化顺序： ',rankdata(std_x))\n",
    "print('归一化顺序: ',rankdata(norm_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征工程归一化有什么作用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![归一化作用](photo/归一化作用.png)\n",
    "\n",
    "> 从上可以看出，数据归一化后，最优解的寻优过程明显会变得平缓，更容易正确的收敛到最优解。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们知道在神经网络训练开始前，都要对输入数据做一个归一化处理，那么具体**为什么需要归一化呢？**归一化后有什么好处呢？原因在于神经网络学习过程本质就是为了学习数据分布，一旦训练数据与测试数据的分布不同，那么网络的泛化能力也大大降低；另外一方面，一旦每批训练数据的分布各不相同(batch 梯度下降)，那么网络就要在每次迭代都去学习适应不同的分布，这样将会大大降低网络的训练速度，这也正是为什么我们需要对数据都要做一个归一化预处理的原因。\n",
    "\n",
    "对于深度网络的训练是一个复杂的过程，只要网络的前面几层发生微小的改变，那么后面几层就会被累积放大下去。一旦网络某一层的输入数据的分布发生改变，那么这一层网络就需要去适应学习这个新的数据分布，所以如果训练过程中，训练数据的分布一直在发生变化，那么将会影响网络的训练速度。\n",
    "\n",
    "我们知道网络一旦train起来，那么参数就要发生更新，除了输入层的数据外(因为输入层数据，我们已经人为的为每个样本归一化)，后面网络每一层的输入数据分布是一直在发生变化的，因为在训练的时候，前面层训练参数的更新将导致后面层输入数据分布的变化。以网络第二层为例：网络的第二层输入，是由第一层的参数和input计算得到的，而第一层的参数在整个训练过程中一直在变化，因此必然会引起后面每一层输入数据分布的改变。我们把网络中间层在训练过程中，数据分布的改变称之为：“Internal  Covariate Shift”。Paper所提出的算法，**就是要解决在训练过程中，中间层数据分布发生改变的情况**，于是就有了**Batch  Normalization**，这个牛逼算法的诞生。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正态分布Box-Cox变换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将一个非正态分布转换为正态分布,使得分布具有对称性\n",
    "\n",
    "![Box-Cox公式](photo/tet.png)\n",
    "![效果图像](photo/picture.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
